# -*- coding: utf-8 -*-
"""Final codee.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bUi3EsPmkD54KHAcEwCEYkeJcQukM1dL

## ***Strategic Customer Profiling and Marketing Enhancement: Integrating Lifetime Value, Churn Risk, and Behavioral Segmentation***

# Part 1: Customer Lifetime Value Analysis(Identifying Long term customers/Loyal Customers)
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from imblearn.over_sampling import SMOTE

# Load Data
file_path = '/content/SAMPLE D.xlsx'
data = pd.read_excel(file_path)

# Create a target variable for loyalty (assuming 'yes' means a loyal customer)
data['Loyal_Customer'] = np.where(data['y'] == 'yes', 1, 0)

# Define features and target
features = ['age', 'duration', 'campaign', 'previous', 'pdays', 'emp_var_rate',
            'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'Inflation', 'M3', 'sofr']
X = data[features]
y = data['Loyal_Customer']

# Split the dataset into training and testing sets (80-20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Apply SMOTE to handle class imbalance
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

# Train a RandomForest Classifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train_resampled, y_train_resampled)

# Cross-validation score to ensure robustness
cv_scores = cross_val_score(model, X_train_resampled, y_train_resampled, cv=5, scoring='f1')

# Predict on the test set
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]  # Probability of the positive class (loyalty)

# Classification Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, zero_division=0)
recall = recall_score(y_test, y_pred, zero_division=0)
f1 = f1_score(y_test, y_pred, zero_division=0)
conf_matrix = confusion_matrix(y_test, y_pred)

# Feature Importance
importances = model.feature_importances_
feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Confidence Interval (CI) using the mean and std of the predicted probabilities across trees
def get_confidence_interval(probs, confidence_level=0.95):
    # Calculate the confidence interval using percentiles
    lower_bound = np.percentile(probs, (1 - confidence_level) / 2 * 100, axis=1)
    upper_bound = np.percentile(probs, (1 + confidence_level) / 2 * 100, axis=1)
    return lower_bound, upper_bound

# Predictive Interval (PI) using the variance of the predicted probabilities
def get_predictive_interval(probs, confidence_level=0.95):
    # Calculate the predictive interval based on the range of predictions across trees
    lower_bound = np.percentile(probs, (1 - confidence_level) / 2 * 100, axis=1)
    upper_bound = np.percentile(probs, (1 + confidence_level) / 2 * 100, axis=1)
    return lower_bound, upper_bound

# Confidence Interval and Predictive Interval
ci_lower, ci_upper = get_confidence_interval(model.predict_proba(X_test), confidence_level=0.95)
pi_lower, pi_upper = get_predictive_interval(model.predict_proba(X_test), confidence_level=0.95)

# Display results
print("\nClassification Metrics for Loyal Customer Prediction:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"Cross-Validation F1 Score (Avg): {cv_scores.mean():.4f}")
print(f"Confusion Matrix:\n{conf_matrix}")

print("\nFeature Importance for Loyalty Prediction:")
print(feature_importance_df)

# Display the CI and PI for the first few test samples
print("\nConfidence Interval (CI) and Predictive Interval (PI) for the first 5 test samples:")
for i in range(5):
    print(f"Sample {i+1}: CI = ({ci_lower[i]:.4f}, {ci_upper[i]:.4f}), PI = ({pi_lower[i]:.4f}, {pi_upper[i]:.4f})")

"""***Interpretation:***

1.The model has an accuracy of 89.61%, meaning it is fairly reliable in distinguishing loyal from non-loyal customers.

2.Precision (52.94%) and Recall (71.84%) indicate that while the model catches most loyal customers (high recall), it has a moderate rate of false positives (precision).

3.Feature Importance Analysis reveals that "Duration" (45.4%) is the most influential predictor, followed by macroeconomic factors like Euribor3m and nr_employed.

Confidence Intervals Visual Representation
"""

import matplotlib.pyplot as plt

# Confidence Interval (CI)
def get_confidence_interval(probs, confidence_level=0.95):
    lower_bound = np.percentile(probs, (1 - confidence_level) / 2 * 100, axis=1)
    upper_bound = np.percentile(probs, (1 + confidence_level) / 2 * 100, axis=1)
    return lower_bound, upper_bound

ci_lower, ci_upper = get_confidence_interval(model.predict_proba(X_test), confidence_level=0.95)
print(ci_lower)
print(ci_upper)

# Sort probabilities for visualization
sorted_indices = np.argsort(y_pred_proba)
sorted_proba = y_pred_proba[sorted_indices]
sorted_ci_lower = ci_lower[sorted_indices]
sorted_ci_upper = ci_upper[sorted_indices]

# Visualization
plt.figure(figsize=(10, 6))

# Plot Upper and Lower CI
plt.plot(range(len(sorted_proba)), sorted_ci_upper, color='purple', linestyle='dashed', label='Upper Confidence Interval')
plt.plot(range(len(sorted_proba)), sorted_ci_lower, color='red', linestyle='dashed', label='Lower Confidence Interval')

# Plot Probability Predictions
#plt.plot(range(len(sorted_proba)), sorted_proba, color='blue', linewidth=2, label='Predicted Probabilities')

# Labels and Legend
plt.xlabel("Samples (Sorted by Probability)")
plt.ylabel("Probability of Loyalty")
plt.title("Confidence Interval for Loyalty Prediction")
plt.legend()
plt.show()

"""***Interpretation:***

1.CI for the samples is (0.025, 0.975), it means that we are confident that the true probability of loyalty lies between 2.5% and 97.5%.

2.On the left side of the graph, the UCI is high (~1.0), meaning those customers are very likely to be loyal.

3.On the right side of the graph, the UCI drops, while the LCI increases. This means uncertainty is highest for mid-range probabilities.

4.In the middle of the graph, where probabilities are around 0.5, both UCI and LCI converge. This indicates that for these customers, the model is less certain in predicting loyalty.

5.the confidence interval (CI) shrinks in the middle, the model is most confident in mid-range predictions but less certain at extreme probabilities.

# Part 2:Churn Analysis(Indentifying Non Converted Customers)
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import statsmodels.formula.api as smf
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve
from sklearn.utils import resample
from joblib import Parallel, delayed

# Load data
df = pd.read_excel('/content/SAMPLE D.xlsx')

# Feature encoding for categorical variables (e.g., marital, poutcome)
df_encoded = pd.get_dummies(df, drop_first=True)

# Ensure the target column 'y' is converted to binary (1 = 'yes', 0 = 'no')
df_encoded['y'] = df['y'].map({'yes': 1, 'no': 0})

# Define the formula: target variable 'y' and predictors
formula = 'y ~ age + duration + campaign +  pdays + previous + poutcome_nonexistent + poutcome_success + marital_married + marital_single'

# Fit the logistic regression model
logit_model = smf.logit(formula=formula, data=df_encoded).fit()

# Print model summary
print(logit_model.summary())

# Get model coefficients
coefficients = logit_model.params
print("\nCoefficients:\n", coefficients)

# ---- CONFIDENCE INTERVALS & PREDICTIONS ----
# Get prediction summary frame (contains mean prediction, standard error, confidence intervals)
predictions = logit_model.get_prediction(df_encoded)
pred_summary = predictions.summary_frame()

# Print available columns in prediction summary (debugging)
print("\nAvailable columns in prediction summary:\n", pred_summary.columns)

# Extract specific columns from the prediction summary
predicted_values = pred_summary['predicted']  # Predicted probabilities (mean prediction)
se_values = pred_summary['se']  # Standard errors of the predictions
ci_lower_values = pred_summary['ci_lower']  # Lower bound of confidence interval
ci_upper_values = pred_summary['ci_upper']  # Upper bound of confidence interval

# Print the first few values of each extracted column
print("\nPredicted Probabilities (First 5 rows):\n", predicted_values.head())
print("\nStandard Errors (First 5 rows):\n", se_values.head())
print("\nConfidence Interval Lower Bound (First 5 rows):\n", ci_lower_values.head())
print("\nConfidence Interval Upper Bound (First 5 rows):\n", ci_upper_values.head())

# Add the predicted probabilities and confidence intervals to the dataframe
df_encoded['predicted_prob'] = predicted_values
df_encoded['CI_lower'] = ci_lower_values
df_encoded['CI_upper'] = ci_upper_values

# Convert probabilities to binary predictions (threshold = 0.5)
df_encoded['predicted_class'] = np.where(df_encoded['predicted_prob'] > 0.5, 1, 0)

# ---- ACCURACY METRICS ----
y_true = df_encoded['y']  # Actual values
y_pred = df_encoded['predicted_class']  # Predicted values

accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred, zero_division=0)
recall = recall_score(y_true, y_pred, zero_division=0)
f1 = f1_score(y_true, y_pred, zero_division=0)
roc_auc = roc_auc_score(y_true, df_encoded['predicted_prob'])
conf_matrix = confusion_matrix(y_true, y_pred)

print("\n Model Evaluation Metrics:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"ROC-AUC Score: {roc_auc:.4f}")
print(f"Confusion Matrix:\n{conf_matrix}")

# ---- ROC Curve ----
fpr, tpr, thresholds = roc_curve(y_true, df_encoded['predicted_prob'])

# Plotting the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', label=f'ROC Curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random Classifier')
plt.xlabel('False Positive Rate (FPR)')
plt.ylabel('True Positive Rate (TPR)')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# ---- LOGISTIC REGRESSION EQUATION ----
logit_equation = f"logit(p) = {coefficients[0]:.4f}"  # Intercept term
for var, coeff in zip(df_encoded.columns[1:], coefficients[1:]):
    logit_equation += f" + {coeff:.4f} * {var}"

print(f"\nPrediction Formula (Logit):\n {logit_equation}")

# ---- BOOTSTRAP FOR PREDICTIVE INTERVAL (PI) USING PARALLEL PROCESSING ----
# Function to perform bootstrap sampling and refitting
def bootstrap_iteration(i, df_encoded, formula):
    # Sample with replacement from the original dataset
    df_resampled = resample(df_encoded, n_samples=len(df_encoded), random_state=i)

    # Refit the model on the resampled data
    model_resampled = smf.logit(formula=formula, data=df_resampled).fit()

    # Make predictions for the original data using the resampled model
    preds_resampled = model_resampled.predict(df_encoded)

    return preds_resampled

# Number of bootstrap iterations (adjustable for speed)
n_iterations = 100  # You can reduce this to 100 or even 50 for faster execution

# Run bootstrap iterations in parallel
bootstrap_preds = Parallel(n_jobs=-1)(delayed(bootstrap_iteration)(i, df_encoded, formula) for i in range(n_iterations))

# Convert to a DataFrame for easier analysis
bootstrap_preds = np.array(bootstrap_preds).T  # Transpose to get predictions for each sample across iterations

# Calculate percentiles (e.g., 2.5% and 97.5% percentiles for a 95% PI)
PI_lower = np.percentile(bootstrap_preds, 2.5, axis=1)
PI_upper = np.percentile(bootstrap_preds, 97.5, axis=1)

# Add Predictive Intervals to the dataframe
df_encoded['PI_lower'] = PI_lower
df_encoded['PI_upper'] = PI_upper

"""***Interpretation:***

1.Duration (Highly Significant, Positive Impact)

The coefficient for duration (0.0039) is positive and highly significant (p-value < 0.001).
Longer call durations increase the probability of conversion, meaning engaged conversations are crucial.

2.Pdays (Highly Significant, Negative Impact)

A negative coefficient (-0.0016) suggests that higher pdays (days since last contact) reduces the likelihood of conversion.
This implies that clients who have not been contacted for a long time are less likely to convert.

3.Campaign (Highly Significant, Negative Impact)

The coefficient is -0.1107, meaning more attempts in the current campaign reduce conversion chances.
This suggests excessive contacts may lead to disengagement.

4.Previous (Highly Significant, Positive Impact)

The coefficient of 0.2846 means that clients who were previously contacted and responded positively are more likely to convert.
Past successful interactions are strong predictors of future engagement.

5.Poutcome (Strong Impact)

If the previous campaign was successful (poutcome_success), the likelihood of conversion increases significantly (0.9281).
If the outcome was nonexistent (poutcome_nonexistent), conversion probability decreases (-0.2254).

6.Age (Moderate Impact, Positive)

A coefficient of 0.0172 suggests that older customers have a slightly higher chance of conversion.


7.ROC Curve (AUC = 0.8708):

A strong model performance with good predictive ability.
The model can effectively distinguish between converting and non-converting clients.

Confidence Intervals Visual Representation
"""

# Sort values for visualization
df_encoded = df_encoded.sort_values(by='predicted_prob')

# ---- VISUALIZATION: UPPER & LOWER CONFIDENCE INTERVALS ----
plt.figure(figsize=(8, 6))
plt.plot(df_encoded.index, df_encoded['CI_upper'], linestyle='dashed', color='purple', label='Upper Confidence Interval')
plt.plot(df_encoded.index, df_encoded['CI_lower'], linestyle='dashed', color='red', label='Lower Confidence Interval')
plt.xlabel('Samples (Sorted by Probability)')
plt.ylabel('Probability of Loyalty')
plt.title('Confidence Interval for Loyalty Prediction')
plt.legend()
plt.show()

"""***Interpretation:***

1.Most predicted probabilities are near 0, with a long tail extending toward higher probabilities.

2.Confidence interval bounds indicate a narrow uncertainty range for most cases.

3.The 95% confidence intervals for most cases range from 0.02 to 0.12, suggesting a high level of certainty in predictions, except for a few outliers with wider intervals.


"""

# ---- Histogram of Predicted Probabilities with Confidence Bands ----
plt.figure(figsize=(8, 6))
sns.histplot(df_encoded['predicted_prob'], kde=True, color='skyblue', bins=20, label='Predicted Probabilities')
plt.axvline(x=df_encoded['CI_lower'].mean(), color='orange', linestyle='--', label='CI Lower Bound')
plt.axvline(x=df_encoded['CI_upper'].mean(), color='red', linestyle='--', label='CI Upper Bound')

plt.xlabel('Predicted Probability')
plt.ylabel('Frequency')
plt.title('Histogram of Predicted Probabilities with Confidence Intervals')
plt.legend()
plt.grid(True)
plt.show()

"""***Interpretation:***

1.Most predicted probabilities are near 0, with a long tail extending toward higher probabilities.

2.Confidence interval bounds indicate a narrow uncertainty range for most cases.

3.The 95% confidence intervals for most cases range from 0.02 to 0.12, suggesting a high level of certainty in predictions, except for a few outliers with wider intervals.

# Part 3:Cluster Analysis(Segementing the customers based on Macro Economic Indicators)
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score
from sklearn.utils import resample

# Load the data
data = pd.read_excel('/content/SAMPLE D.xlsx')

# Selecting features for clustering
segmentation_features = ['euribor3m', 'sofr', 'Inflation']

# Standardizing the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(data[segmentation_features])

# Finding the optimal number of clusters using the Elbow Method
wcss = []
for i in range(1, 11):  # Testing from 1 to 10 clusters
    kmeans = KMeans(n_clusters=i, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

# Plot Elbow Method
plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), wcss, marker='o', linestyle='-', color='b')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS (Within-Cluster Sum of Squares)')
plt.title('Elbow Method to Determine Optimal Clusters')
plt.grid(True)
plt.show()

# Assuming k=3 based on the elbow method
optimal_k = 3
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
data['Cluster'] = kmeans.fit_predict(X_scaled)

# Bootstrapping to estimate variability of centroids
def bootstrap_kmeans(X, n_clusters, n_iterations=100):
    centroids_list = []
    for i in range(n_iterations):
        # Sample with replacement from the original data
        X_resampled = resample(X, replace=True)

        # Perform KMeans on the resampled data
        kmeans = KMeans(n_clusters=n_clusters, random_state=42)
        kmeans.fit(X_resampled)

        # Store the centroids of the current iteration
        centroids_list.append(kmeans.cluster_centers_)

    return np.array(centroids_list)

# Bootstrapping centroids for 100 iterations
bootstrap_centroids = bootstrap_kmeans(X_scaled, optimal_k, n_iterations=100)

# Calculate the confidence intervals (2.5% and 97.5% percentiles for 95% CI)
centroid_lower = np.percentile(bootstrap_centroids, 2.5, axis=0)
centroid_upper = np.percentile(bootstrap_centroids, 97.5, axis=0)

# Print the confidence intervals for each centroid
print("Confidence Intervals for Cluster Centroids:")
for i in range(optimal_k):
    print(f"Cluster {i+1}:")
    print(f"  Centroid: {kmeans.cluster_centers_[i]}")
    print(f"  CI Lower: {centroid_lower[i]}")
    print(f"  CI Upper: {centroid_upper[i]}")

# Evaluating the performance
# Silhouette Score
sil_score = silhouette_score(X_scaled, data['Cluster'])
print(f"Silhouette Score: {sil_score:.4f}")

# Davies-Bouldin Index
db_index = davies_bouldin_score(X_scaled, data['Cluster'])
print(f"Davies-Bouldin Index: {db_index:.4f}")

# Inertia (WCSS)
inertia = kmeans.inertia_
print(f"Inertia (WCSS): {inertia:.4f}")

# Visualizing clusters (Scatter plot of Euribor3m vs SOFR)
plt.figure(figsize=(8, 6))
sns.scatterplot(data=data, x='euribor3m', y='sofr', hue='Cluster', palette='viridis')
plt.xlabel('Euribor 3M')
plt.ylabel('SOFR')
plt.title('Customer Segments Based on Euribor3M and SOFR')
plt.legend(title="Cluster")
plt.grid(True)
plt.show()

# Visualizing clusters (Scatter plot of Inflation vs Euribor3m)
plt.figure(figsize=(8, 6))
sns.scatterplot(data=data, x='Inflation', y='euribor3m', hue='Cluster', palette='coolwarm')
plt.xlabel('Inflation')
plt.ylabel('Euribor 3M')
plt.title('Customer Segments Based on Inflation and Euribor3M')
plt.legend(title="Cluster")
plt.grid(True)
plt.show()

# Cluster profiling - Understanding characteristics of each segment
cluster_profile = data.groupby('Cluster')[segmentation_features].describe()
print("Cluster Profile:\n", cluster_profile)

# Count of customers in each cluster
cluster_counts = data['Cluster'].value_counts()
print("\nCluster Size Distribution:\n", cluster_counts)

# Visualizing the distribution of cluster characteristics
plt.figure(figsize=(10, 6))
for feature in segmentation_features:
    sns.boxplot(x='Cluster', y=feature, data=data)
    plt.title(f'Distribution of {feature} Across Clusters')
    plt.xlabel('Cluster')
    plt.ylabel(feature)
    plt.grid(True)
    plt.show()

# Visualizing Cluster Centroids with Confidence Intervals as Barplots
centroid_means = kmeans.cluster_centers_



# Plot Cluster Centroids with 95% Confidence Intervals
fig, ax = plt.subplots(figsize=(10, 6))
bar_width = 0.2
colors = ['teal', 'purple', 'grey']  # Euribor3M, SOFR, Inflation
labels = ['Euribor 3M', 'SOFR', 'Inflation']

for i in range(optimal_k):
    for j in range(3):  # Three features
        ax.bar(i + (j - 1) * bar_width, centroid_means[i, j], width=bar_width, color=colors[j],
               label=f'Cluster {i+1} {labels[j]}' if i == 0 else "",
               yerr=[[centroid_means[i, j] - centroid_lower[i, j]], [centroid_upper[i, j] - centroid_means[i, j]]],
               capsize=5)

ax.set_xticks(range(optimal_k))
ax.set_xticklabels([f'Cluster {i+1}' for i in range(optimal_k)])
ax.set_ylabel('Centroid Value')
ax.set_title('Cluster Centroids with 95% Confidence Intervals')
ax.legend(loc='upper left')
plt.grid(True)
plt.show()

"""***Interpretation:***

1.The confidence intervals suggest the reliability of the cluster centroids.
2.The silhouette score (0.6499) and Davies-Bouldin Index (0.5496) indicate moderately well-separated clusters.
3.Cluster 0 has the highest average Euribor 3M (4.92) and a high standard deviation in inflation (21.31).
4.The larger the error bars, the more variation exists in the cluster’s financial attributes.
Cluster 3 has the widest error bars, meaning more variability among clients in their exposure to inflation and SOFR.
Cluster 2 shows a more stable trend, with smaller confidence intervals for inflation.

Deposit Ccount Distribution among clusters
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Load the data
data = pd.read_excel('/content/SAMPLE D.xlsx')

# Selecting features for clustering
segmentation_features = ['euribor3m', 'sofr', 'Inflation']

# Standardizing the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(data[segmentation_features])

# Applying K-Means clustering (assuming k=3 based on elbow method)
optimal_k = 3
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
data['Cluster'] = kmeans.fit_predict(X_scaled)

# Count of "yes" and "no" in each cluster
yes_no_distribution = data.groupby('Cluster')['y'].value_counts(normalize=True).unstack() * 100  # Convert to percentage

# Visualization of "yes" vs. "no" in each cluster
yes_no_distribution.plot(kind='bar', stacked=True, figsize=(8, 6), colormap='viridis')
plt.title('Distribution of "Yes" (Converted) and "No" (Not Converted) Across Clusters')
plt.xlabel('Cluster')
plt.ylabel('Percentage')
plt.legend(title="Response", labels=["No", "Yes"])
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

# Print the distribution
print("Yes/No Distribution in Clusters (%):\n", yes_no_distribution)

"""Interpretation:

1.Cluster 0 has the lowest conversion rate (4.61%), while Clusters 1 and 2 show significantly higher rates (21.15% and 19.31% respectively).


"""